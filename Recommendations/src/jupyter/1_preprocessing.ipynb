{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing participatory budgeting data for Cambridge, Miami and New York"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Before we start with the recommendation algorithms, we need to prepare the data. Firstly, we create a matrix with the users and the proposals they have commented on. This matrix will reflex how many times a user has commented on a proposal.\n",
    "This matrix will be defined as follows: $M_{u,p}$ is the number of comments that user $u$ has made on proposal $p$. This matrix will be sparse, as most users have not commented on most proposals, and will be saved in a sparse matrix format to save memory. As we have a lot of different databases, we need to define the structure of the directories where the data is stored. We will define the following structure:\n",
    "- `data\\`: directory where the data is saved\n",
    "    - `data\\rm\\`: directory where we save the recommendation matrices.\n",
    "        - `data\\rm\\city\\year\\`: directory where we save the recommendation matrices for a city and a year.\n",
    "\n",
    "The filenames will be defined as follows:\n",
    "- `data\\rm\\city\\year\\num_comm_matrix.npz`: sparse matrix with the number of comments that each user has made on each proposal.\n",
    "- `data\\rm\\city\\year\\num_comm_train.npz`: sparse matrix with the number of comments that each user has made on each proposal in the training set.\n",
    "- `data\\rm\\city\\year\\num_comm_test.npz`: sparse matrix with the number of comments that each user has made on each proposal in the test set.\n",
    "- `data\\rm\\city\\year\\user_mapping.csv`: mapping between each userId and its index in the matrix.\n",
    "- `data\\rm\\city\\year\\item_mapping.csv`: mapping between each itemId and its index in the matrix.\n",
    "\n",
    "**Remember**: in our database, itemId refers to the proposals and userId refers to the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from scipy.sparse import load_npz, csr_matrix, save_npz\n",
    "import scipy.sparse as sps\n",
    "# DB connection and path modification\n",
    "import sys\n",
    "import os\n",
    "import pymysql\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#implicit\n",
    "from implicit.evaluation import train_test_split\n",
    "from implicit.cpu.lmf import LogisticMatrixFactorization\n",
    "from implicit.cpu.bpr import BayesianPersonalizedRanking\n",
    "from implicit.nearest_neighbours import CosineRecommender\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    python_path = os.path.join(module_path, 'python')\n",
    "    sys.path.append(python_path)\n",
    "\n",
    "# Recommenders\n",
    "from implicit_extend.popularity import PopularityRecommender, PopularityNumCommentsRecommender\n",
    "from implicit_extend.random import RandomRecommender\n",
    "from implicit_extend.nearest_neighbours_ub import CosineRecommenderUB\n",
    "from implicit_extend.content_based import ContentBasedRecommender\n",
    "from implicit_extend.evaluation import ranking_metrics_at_k\n",
    "from implicit_extend.hybrid import HybridRecommenderUB, HybridRecommenderIB\n",
    "# Auxiliary functions\n",
    "from dusa_function_lib import get_rm_train_test_info, build_db_name\n",
    "from dusa_function_lib import build_directory_city_name\n",
    "from dusa_function_lib import tunning_and_metrics\n",
    "from dusa_function_lib import gen_recommendations\n",
    "from dusa_function_lib import get_n_for_ndcg\n",
    "from dusa_function_lib import get_item_category_info, get_item_location_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql database connection\n",
    "connection = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='eduardomv',\n",
    "    password='*****',\n",
    "    database='participatory_budgeting'\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cities = ['Cambridge', 'Miami', 'New York']\n",
    "years = ['2014', '2015', '2016', '2017']\n",
    "for city in cities:\n",
    "    city_directory_name = build_directory_city_name(city)\n",
    "    for year in years:\n",
    "        database_name = build_db_name(city, year)\n",
    "        query = f\"\"\"\n",
    "        SELECT userId, itemId, COUNT(*) as num_comments\n",
    "        FROM \n",
    "            ratings r\n",
    "        WHERE \n",
    "            dataset = '{database_name}'\n",
    "        GROUP BY userId, itemId\n",
    "        ORDER BY userId ASC, itemId ASC;\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        data = cursor.fetchall()\n",
    "        \n",
    "        user_ids = []\n",
    "        proposal_ids = []\n",
    "        num_comments = []\n",
    "        \n",
    "        for row in data:\n",
    "            user_ids.append(row[0])\n",
    "            proposal_ids.append(row[1])\n",
    "            num_comments.append(row[2])\n",
    "    \n",
    "        unique_user_ids = {uid: id_enum for id_enum, uid in enumerate(pd.unique(user_ids))}\n",
    "        unique_proposal_ids = {pid: idx for idx, pid in enumerate(pd.unique(proposal_ids))}\n",
    "        \n",
    "        row_indices = [unique_user_ids[uid] for uid in user_ids] # looks for the new value of the user id\n",
    "        col_indices = [unique_proposal_ids[pid] for pid in proposal_ids]\n",
    "        \n",
    "        # Create the sparse matrix\n",
    "        matrix = csr_matrix(\n",
    "            (num_comments, (row_indices, col_indices)),\n",
    "            shape=(len(unique_user_ids), len(unique_proposal_ids))\n",
    "        )\n",
    "        \n",
    "        city_year_directory_rm_name = f\"../../data/rm/{city_directory_name}/{year}\"\n",
    "        save_npz(f\"{city_year_directory_rm_name}/num_comm_matrix.npz\", matrix)\n",
    "        print(f\"Matrix {database_name} has shape: {matrix.shape}\")\n",
    "        \n",
    "        user_mapping = pd.DataFrame(list(unique_user_ids.items()), columns=['userId', 'new_userId'])\n",
    "        proposal_mapping = pd.DataFrame(list(unique_proposal_ids.items()), columns=['itemId', 'new_itemId'])\n",
    "        \n",
    "        user_mapping.to_csv(f\"{city_year_directory_rm_name}/user_mapping.csv\", index=False, sep=\"|\")\n",
    "        proposal_mapping.to_csv(f\"{city_year_directory_rm_name}/item_mapping.csv\", index=False, sep=\"|\")\n",
    "\n",
    "        # Split the matrix data into train and test\n",
    "        mat_train, mat_test = train_test_split(matrix, train_percentage=0.8, random_state=1)\n",
    "        mat_train = mat_train.astype(float)\n",
    "        mat_test = mat_test.astype(float)\n",
    "        \n",
    "        save_npz(f\"{city_year_directory_rm_name}/num_comm_train.npz\", mat_train)\n",
    "        save_npz(f\"{city_year_directory_rm_name}/num_comm_test.npz\", mat_test)\n",
    "        \n",
    "cursor.close()\n",
    "connection.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create some extra files that will help us to work with the data. As before, we will store this data in a sparse matrix format to save memory. The files we will create are:\n",
    "- `data\\category\\city\\year\\item_category_matrix.npz`: sparse matrix with the categories of each proposal. The proposals will be stored in the rows and the categories in the columns. The value of the matrix will be 1 if the proposal belongs to the category and 0 otherwise.\n",
    "- `data\\category\\city\\year\\it_cat_category_mapping.csv`: mapping between each category and its index in the matrix.\n",
    "- `data\\category\\city\\year\\it_cat_item_mapping.csv`: mapping between each itemId and its index in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql database connection\n",
    "connection = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='eduardomv',\n",
    "    password='*****',\n",
    "    database='participatory_budgeting'\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cities = ['Cambridge', 'Miami', 'New York']\n",
    "years = ['2014', '2015', '2016', '2017']\n",
    "for city in cities:\n",
    "    city_directory_name = build_directory_city_name(city)\n",
    "    for year in years:\n",
    "        database_name = build_db_name(city, year)\n",
    "        query = f\"\"\"\n",
    "        SELECT id, category\n",
    "        FROM\n",
    "            items it\n",
    "        WHERE \n",
    "            dataset = '{database_name}'\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        data = cursor.fetchall()\n",
    "        \n",
    "        proposal_ids = []\n",
    "        categories = []\n",
    "        category_in_proposal = []\n",
    "        \n",
    "        for row in data:\n",
    "            proposal_ids.append(row[0])\n",
    "            categories.append(row[1])\n",
    "            category_in_proposal.append(1)\n",
    "    \n",
    "        unique_proposal_ids = {pid: idx for idx, pid in enumerate(pd.unique(proposal_ids))}\n",
    "        unique_categories_ids = {cat: idx for idx, cat in enumerate(pd.unique(categories))}\n",
    "        \n",
    "        row_indices = [unique_proposal_ids[pid] for pid in proposal_ids]\n",
    "        col_indices = [unique_categories_ids[cat] for cat in categories]\n",
    "        \n",
    "        # Create the sparse matrix\n",
    "        matrix = csr_matrix(\n",
    "            (category_in_proposal, (row_indices, col_indices)),\n",
    "            shape=(len(unique_proposal_ids), len(unique_categories_ids))\n",
    "        )\n",
    "        \n",
    "        city_year_directory_rm_name = f\"../../data/category/{city_directory_name}/{year}\"\n",
    "        save_npz(f\"{city_year_directory_rm_name}/item_category_matrix.npz\", matrix)\n",
    "        print(f\"Matrix item_category from {database_name} has shape: {matrix.shape}\")\n",
    "        \n",
    "        proposal_mapping = pd.DataFrame(list(unique_proposal_ids.items()), columns=['itemId', 'new_itemId'])\n",
    "        category_mapping = pd.DataFrame(list(unique_categories_ids.items()), columns=['category', 'new_categoryId'])\n",
    "        \n",
    "        proposal_mapping.to_csv(f\"{city_year_directory_rm_name}/it_cat_item_mapping.csv\", index=False, sep=\"|\")\n",
    "        category_mapping.to_csv(f\"{city_year_directory_rm_name}/it_cat_category_mapping.csv\", index=False, sep=\"|\")\n",
    "        \n",
    "        \n",
    "cursor.close()\n",
    "connection.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering to generate \"Neighborhoods\"\n",
    "In this section we will create the neighborhoods for each city. These neighborhoods will be used in the location recommendation algorithms. They will be created using the KMeans algorithm. The number of clusters will be defined depending on the silhouette score. We will save the neighborhoods in the following format:\n",
    "- `../data/location/city/clusters_city.csv`: csv file with the clusters for each city. The file will have the following structure:\n",
    "    - `dataset`: name of the dataset (city-year)\n",
    "    - `id`: id of the proposal\n",
    "    - `latitude`: latitude of the proposal\n",
    "    - `longitude`: longitude of the proposal\n",
    "    - `cluster`: id of the cluster\n",
    "Additionally, we will create a map in html in order to visualize the clusters. It will be saved in the following directory:\n",
    "- `../../data/location/city/mapa_clusters_city.html`: html file with the map of the clusters.\n",
    "\n",
    "After generating the `csv` files, we will create some sparse matrices that will be used in the location recommendation algorithms. This sparse matrices will have itemId in the rows and the clusters in the columns. The value of the matrix will be 1 if the proposal belongs to the cluster and 0 otherwise. We will save the matrices in the following files:\n",
    "- `../../data/location/city/year/item_cluster_matrix.npz`: sparse matrix with the clusters of each proposal of that city and year.\n",
    "- `../../data/location/city/year/it_loc_item_mapping.csv`: mapping between each itemId and its index in the matrix.\n",
    "- `../../data/location/city/year/it_loc_cluster_mapping.csv`: mapping between each cluster and its index in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pymysql\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import folium\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from implicit.evaluation import train_test_split\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    python_path = os.path.join(module_path, 'python')\n",
    "    sys.path.append(python_path)\n",
    "\n",
    "from dusa_function_lib import build_directory_city_name, build_db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número óptimo de clústers según silhouette: 8 (score=0.42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\molin\\AppData\\Local\\Temp\\ipykernel_19268\\1473451583.py:71: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colormap = cm.get_cmap('tab10', best_k)  # 'tab10', 'Set1', etc. ajustables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número óptimo de clústers según silhouette: 6 (score=0.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\molin\\AppData\\Local\\Temp\\ipykernel_19268\\1473451583.py:71: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colormap = cm.get_cmap('tab10', best_k)  # 'tab10', 'Set1', etc. ajustables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número óptimo de clústers según silhouette: 8 (score=0.49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\molin\\AppData\\Local\\Temp\\ipykernel_19268\\1473451583.py:71: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colormap = cm.get_cmap('tab10', best_k)  # 'tab10', 'Set1', etc. ajustables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# mysql database connection\n",
    "connection = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='eduardomv',\n",
    "    password='*****',\n",
    "    database='participatory_budgeting'\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cities = ['Cambridge', 'Miami', 'New York']\n",
    "years = ['2014', '2015', '2016', '2017']\n",
    "for city in cities:\n",
    "    db_city_names = []\n",
    "    city_directory_name = build_directory_city_name(city)\n",
    "    for year in years:\n",
    "        db_city_names.append(build_db_name(city, year))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT dataset, id, longitude, latitude\n",
    "    FROM \n",
    "        items\n",
    "    WHERE \n",
    "        dataset = '{db_city_names[0]}' OR dataset = '{db_city_names[1]}' OR dataset = '{db_city_names[2]}' OR dataset = '{db_city_names[3]}';\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    data = cursor.fetchall()\n",
    "    \n",
    "    databases = []\n",
    "    user_ids = []\n",
    "    longitudes = []\n",
    "    latitudes = []\n",
    "    \n",
    "    for row in data:\n",
    "        databases.append(row[0])\n",
    "        user_ids.append(row[1])\n",
    "        longitudes.append(row[2])\n",
    "        latitudes.append(row[3])\n",
    "    \n",
    "    df = pd.DataFrame({'dataset':databases, 'id': user_ids, 'latitude': latitudes, 'longitude': longitudes})\n",
    "\n",
    "    coords = df[['latitude', 'longitude']].values\n",
    "\n",
    "    # Determine the optimal number of clusters using silhouette score\n",
    "    max_k = 10\n",
    "    best_k = 4\n",
    "    best_score = -1\n",
    "\n",
    "    for k in range(4, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(coords)\n",
    "        labels_temp = kmeans.labels_\n",
    "        sil_score = silhouette_score(coords, labels_temp)\n",
    "        if sil_score > best_score:\n",
    "            best_score = sil_score\n",
    "            best_k = k\n",
    "\n",
    "    print(f\"Número óptimo de clústers según silhouette: {best_k} (score={best_score:.2f})\")\n",
    "\n",
    "    kmeans_final = KMeans(n_clusters=best_k, random_state=42)\n",
    "    df['cluster'] = kmeans_final.fit_predict(coords)\n",
    "\n",
    "    center_lat = df['latitude'].mean()\n",
    "    center_lon = df['longitude'].mean()\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=14)\n",
    "\n",
    "    colormap = cm.get_cmap('tab10', best_k)  # 'tab10', 'Set1', etc. ajustables\n",
    "    cluster_colors = {}\n",
    "    for cl in range(best_k):\n",
    "        rgba = colormap(cl)[:3]\n",
    "        hexcolor = '#{:02x}{:02x}{:02x}'.format(int(rgba[0]*255),\n",
    "                                            int(rgba[1]*255),\n",
    "                                            int(rgba[2]*255))\n",
    "        cluster_colors[cl] = hexcolor\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        cl = row['cluster']\n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=5,\n",
    "            color=cluster_colors[cl],\n",
    "            fill=True,\n",
    "            fill_color=cluster_colors[cl],\n",
    "            fill_opacity=0.8,\n",
    "        ).add_to(m)\n",
    "\n",
    "    legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "        bottom: 50px; left: 50px; width: 150px; height: auto; \n",
    "        border:2px solid grey; z-index:9999; font-size:14px;\">\n",
    "    &nbsp;<b>Clusters</b><br/>\n",
    "    '''\n",
    "    for cl in range(best_k):\n",
    "        legend_html += f'''&nbsp;<i style=\"background:{cluster_colors[cl]};width:12px;height:12px;\n",
    "                        display:inline-block;\"></i>\n",
    "                        &nbsp;Cluster {cl}<br/>'''\n",
    "    legend_html += '</div>'\n",
    "\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "    m.save(f\"../../data/location/{city_directory_name}/mapa_clusters_{city_directory_name}.html\")\n",
    "    df.to_csv(f\"../../data/location/{city_directory_name}/clusters_{city_directory_name}.csv\", index=False)\n",
    "    \n",
    "        \n",
    "cursor.close()\n",
    "connection.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\molin\\AppData\\Local\\Temp\\ipykernel_19268\\3751535391.py:10: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_proposals_ids = {pid: idx for idx, pid in enumerate(pd.unique(proposal_ids))}\n",
      "C:\\Users\\molin\\AppData\\Local\\Temp\\ipykernel_19268\\3751535391.py:11: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_proposals_clusters = {clus: idx for idx, clus in enumerate(pd.unique(proposals_clusters))}\n",
      "C:\\Users\\molin\\AppData\\Local\\Temp\\ipykernel_19268\\3751535391.py:10: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_proposals_ids = {pid: idx for idx, pid in enumerate(pd.unique(proposal_ids))}\n",
      "C:\\Users\\molin\\AppData\\Local\\Temp\\ipykernel_19268\\3751535391.py:11: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_proposals_clusters = {clus: idx for idx, clus in enumerate(pd.unique(proposals_clusters))}\n",
      "C:\\Users\\molin\\AppData\\Local\\Temp\\ipykernel_19268\\3751535391.py:10: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_proposals_ids = {pid: idx for idx, pid in enumerate(pd.unique(proposal_ids))}\n",
      "C:\\Users\\molin\\AppData\\Local\\Temp\\ipykernel_19268\\3751535391.py:11: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_proposals_clusters = {clus: idx for idx, clus in enumerate(pd.unique(proposals_clusters))}\n"
     ]
    }
   ],
   "source": [
    "for city in cities:\n",
    "    city_directory_name = build_directory_city_name(city)\n",
    "    df = pd.read_csv(f\"../../data/location/{city_directory_name}/clusters_{city_directory_name}.csv\")\n",
    "    for year in years:\n",
    "        database_name = build_db_name(city, year)\n",
    "        df_year = df[df['dataset'] == database_name]\n",
    "        proposal_ids = df_year['id'].tolist()\n",
    "        proposals_clusters = df_year['cluster'].tolist()\n",
    "        \n",
    "        unique_proposals_ids = {pid: idx for idx, pid in enumerate(pd.unique(proposal_ids))}\n",
    "        unique_proposals_clusters = {clus: idx for idx, clus in enumerate(pd.unique(proposals_clusters))}\n",
    "        \n",
    "        row_indices = [unique_proposals_ids[pid] for pid in proposal_ids]\n",
    "        col_indices = [unique_proposals_clusters[clus] for clus in proposals_clusters]\n",
    "        \n",
    "        # crear lista de unos de la longitud de la cantidad de propuestas\n",
    "        cluster_in_proposals = [1] * len(proposal_ids)\n",
    "        \n",
    "        matrix = csr_matrix(\n",
    "            (cluster_in_proposals, (row_indices, col_indices)),\n",
    "            shape=(len(unique_proposals_ids), len(unique_proposals_clusters))\n",
    "        )\n",
    "        \n",
    "        city_year_location_name = f\"../../data/location/{city_directory_name}/{year}\"\n",
    "        save_npz(f\"{city_year_location_name}/item_cluster_matrix.npz\", matrix) \n",
    "        proposal_mapping = pd.DataFrame(list(unique_proposals_ids.items()), columns=['itemId', 'new_itemId'])\n",
    "        cluster_mapping = pd.DataFrame(list(unique_proposals_clusters.items()), columns=['clusterId', 'new_clusterId'])\n",
    "        proposal_mapping.to_csv(f\"{city_year_location_name}/it_loc_item_mapping.csv\", index=False, sep='|')\n",
    "        cluster_mapping.to_csv(f\"{city_year_location_name}/it_loc_cluster_mapping.csv\", index=False, sep='|')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
